# Default values for mojaloop-efk.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  config:
    ## Pod scheduling preferences.
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ##
    affinity: {}

    ## Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}

    ## Set toleration for schedular
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []

# Declare variables to be passed into your templates.
fluentd-elasticsearch:
  image:
    repository: quay.io/fluentd_elasticsearch/fluentd
    ## Specify an imagePullPolicy (Required)
    ## It's recommended to change this to 'Always' if the image tag is 'latest'
    ## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
    tag: v2.6.0
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistrKeySecretName

  # Specify to use specific priorityClass for pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  # If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority
  # Pods to make scheduling of the pending Pod possible.
  priorityClassName: ""

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 500Mi
    # requests:
  #   cpu: 100m
  #   memory: 200Mi

  elasticsearch:
    host: 'elasticsearch-master'
    port: 9200
    scheme: 'http'
    ssl_version: TLSv1_2
    buffer_chunk_limit: 2M
    buffer_queue_limit: 8
    logstash_prefix: 'fluentd'

  # If you want to add custom environment variables, use the env dict
  # You can then reference these in your config file e.g.:
  #     user "#{ENV['OUTPUT_USER']}"
  env:
  # OUTPUT_USER: my_user

  # If you want to add custom environment variables from secrets, use the secret list
  secret:
  # - name: ELASTICSEARCH_PASSWORD
  #   secret_name: elasticsearch
  #   secret_key: password

  rbac:
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    enabled: false
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  livenessProbe:
    enabled: true

  annotations: {}

  podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "24231"

  ## DaemonSet update strategy
  ## Ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
  updateStrategy:
    type: RollingUpdate

  tolerations: {}
    # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

  nodeSelector: {}

  service: {}
    # type: ClusterIP
    # ports:
  #   - name: "monitor-agent"
  #     port: 24231

  configMaps:
    system.conf: |-
      <system>
        root_dir /tmp/fluentd-buffers/
      </system>
#    containers.input.conf: |-
#      # This configuration file for Fluentd / td-agent is used
#      # to watch changes to Docker log files. The kubelet creates symlinks that
#      # capture the pod name, namespace, container name & Docker container ID
#      # to the docker logs for pods in the /var/log/containers directory on the host.
#      # If running this fluentd configuration in a Docker container, the /var/log
#      # directory should be mounted in the container.
#      #
#      # These logs are then submitted to Elasticsearch which assumes the
#      # installation of the fluent-plugin-elasticsearch & the
#      # fluent-plugin-kubernetes_metadata_filter plugins.
#      # See https://github.com/uken/fluent-plugin-elasticsearch &
#      # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
#      # more information about the plugins.
#      #
#      # Example
#      # =======
#      # A line in the Docker log file might look like this JSON:
#      #
#      # {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
#      #  "stream":"stderr",
#      #   "time":"2014-09-25T21:15:03.499185026Z"}
#      #
#      # The time_format specification below makes sure we properly
#      # parse the time format produced by Docker. This will be
#      # submitted to Elasticsearch and should appear like:
#      # $ curl 'http://elasticsearch-logging:9200/_search?pretty'
#      # ...
#      # {
#      #      "_index" : "logstash-2014.09.25",
#      #      "_type" : "fluentd",
#      #      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
#      #      "_score" : 1.0,
#      #      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
#      #                 "stream":"stderr","tag":"docker.container.all",
#      #                 "@timestamp":"2014-09-25T22:45:50+00:00"}
#      #    },
#      # ...
#      #
#      # The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
#      # record & add labels to the log record if properly configured. This enables users
#      # to filter & search logs on any metadata.
#      # For example a Docker container's logs might be in the directory:
#      #
#      #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
#      #
#      # and in the file:
#      #
#      #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#      #
#      # where 997599971ee6... is the Docker ID of the running container.
#      # The Kubernetes kubelet makes a symbolic link to this file on the host machine
#      # in the /var/log/containers directory which includes the pod name and the Kubernetes
#      # container name:
#      #
#      #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#      #    ->
#      #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#      #
#      # The /var/log directory on the host is mapped to the /var/log directory in the container
#      # running this instance of Fluentd and we end up collecting the file:
#      #
#      #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#      #
#      # This results in the tag:
#      #
#      #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#      #
#      # The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
#      # which are added to the log message as a kubernetes field object & the Docker container ID
#      # is also added under the docker field object.
#      # The final tag is:
#      #
#      #   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#      #
#      # And the final log record look like:
#      #
#      # {
#      #   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
#      #   "stream":"stderr",
#      #   "time":"2014-09-25T21:15:03.499185026Z",
#      #   "kubernetes": {
#      #     "namespace": "default",
#      #     "pod_name": "synthetic-logger-0.25lps-pod",
#      #     "container_name": "synth-lgr"
#      #   },
#      #   "docker": {
#      #     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
#      #   }
#      # }
#      #
#      # This makes it easier for users to search for logs by pod name or by
#      # the name of the Kubernetes container regardless of how many times the
#      # Kubernetes pod has been restarted (resulting in a several Docker container IDs).
#      # Json Log Example:
#      # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
#      # CRI Log Example:
#      # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
#
#      ## Source for all container logs. Fluentd will be running as a Daemonset, and should have access to this location
#      <source>
#        @id fluentd-containers.log
#        @type tail
#        path /var/log/containers/*.log
#        pos_file /var/log/fluentd-containers.log.pos
#        time_format %Y-%m-%dT%H:%M:%S.%NZ
#        tag raw.kubernetes.*
#        format json
#        read_from_head true
#      </source>
#
#      ## Uncomment if you want to ignore all fluentd logs
#      <match kubernetes.var.log.containers.**fluentd**.log>
#        @type null
#      </match>
#
#
#      ## Uncomment if you want to ignore all Kubernetes System logs
#      <match kubernetes.var.log.containers.**kube-system**.log>
#        @type null
#      </match>
#
#      ## Uncomment if you have issues consuming logs
#      #  <match kubernetes.**>
#      #    @type stdout
#      #  </match>
#
#      ## Detect exceptions in the log output and forward them as one log entry.
#      <match raw.kubernetes.**>
#        @id raw.kubernetes
#        @type detect_exceptions
#        remove_tag_prefix raw
#        message log
#        stream stream
#        multiline_flush_interval 5
#        max_bytes 500000
#        max_lines 1000
#      </match>
  #    system.input.conf: |-
  #      # Example:
  #      # 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081
  #      <source>
  #        @id minion
  #        @type tail
  #        format /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
  #        time_format %Y-%m-%d %H:%M:%S
  #        path /var/log/salt/minion
  #        pos_file /var/log/salt.pos
  #        tag salt
  #      </source>
  #      # Example:
  #      # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
  #      <source>
  #        @id startupscript.log
  #        @type tail
  #        format syslog
  #        path /var/log/startupscript.log
  #        pos_file /var/log/startupscript.log.pos
  #        tag startupscript
  #      </source>
  #      # Examples:
  #      # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
  #      # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
  #      <source>
  #        @id docker.log
  #        @type tail
  #        format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
  #        path /var/log/docker.log
  #        pos_file /var/log/docker.log.pos
  #        tag docker
  #      </source>
  #      # Example:
  #      # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
  #      <source>
  #        @id etcd.log
  #        @type tail
  #        # Not parsing this, because it doesn't have anything particularly useful to
  #        # parse out of it (like severities).
  #        format none
  #        path /var/log/etcd.log
  #        pos_file /var/log/etcd.log.pos
  #        tag etcd
  #      </source>
  #      # Multi-line parsing is required for all the kube logs because very large log
  #      # statements, such as those that include entire object bodies, get split into
  #      # multiple lines by glog.
  #      # Example:
  #      # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
  #      <source>
  #        @id kubelet.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/kubelet.log
  #        pos_file /var/log/kubelet.log.pos
  #        tag kubelet
  #      </source>
  #      # Example:
  #      # I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
  #      <source>
  #        @id kube-proxy.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/kube-proxy.log
  #        pos_file /var/log/kube-proxy.log.pos
  #        tag kube-proxy
  #      </source>
  #      # Example:
  #      # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
  #      <source>
  #        @id kube-apiserver.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/kube-apiserver.log
  #        pos_file /var/log/kube-apiserver.log.pos
  #        tag kube-apiserver
  #      </source>
  #      # Example:
  #      # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
  #      <source>
  #        @id kube-controller-manager.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/kube-controller-manager.log
  #        pos_file /var/log/kube-controller-manager.log.pos
  #        tag kube-controller-manager
  #      </source>
  #      # Example:
  #      # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
  #      <source>
  #        @id kube-scheduler.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/kube-scheduler.log
  #        pos_file /var/log/kube-scheduler.log.pos
  #        tag kube-scheduler
  #      </source>
  #      # Example:
  #      # I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler
  #      <source>
  #        @id rescheduler.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/rescheduler.log
  #        pos_file /var/log/rescheduler.log.pos
  #        tag rescheduler
  #      </source>
  #      # Example:
  #      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
  #      <source>
  #        @id glbc.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/glbc.log
  #        pos_file /var/log/glbc.log.pos
  #        tag glbc
  #      </source>
  #      # Example:
  #      # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
  #      <source>
  #        @id cluster-autoscaler.log
  #        @type tail
  #        format multiline
  #        multiline_flush_interval 5s
  #        format_firstline /^\w\d{4}/
  #        format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  #        time_format %m%d %H:%M:%S.%N
  #        path /var/log/cluster-autoscaler.log
  #        pos_file /var/log/cluster-autoscaler.log.pos
  #        tag cluster-autoscaler
  #      </source>
  #      # Logs from systemd-journal for interesting services.
  #      <source>
  #        @id journald-docker
  #        @type systemd
  #        matches [{ "_SYSTEMD_UNIT": "docker.service" }]
  #        <storage>
  #          @type local
  #          persistent true
  #          path /var/log/journald-docker.pos
  #        </storage>
  #        read_from_head true
  #        tag docker
  #      </source>
  #      <source>
  #        @id journald-kubelet
  #        @type systemd
  #        matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
  #        <storage>
  #          @type local
  #          persistent true
  #          path /var/log/journald-kubelet.pos
  #        </storage>
  #        read_from_head true
  #        tag kubelet
  #      </source>
  #      <source>
  #        @id journald-node-problem-detector
  #        @type systemd
  #        matches [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
  #        <storage>
  #          @type local
  #          persistent true
  #          path /var/log/journald-node-problem-detector.pos
  #        </storage>
  #        read_from_head true
  #        tag node-problem-detector
  #      </source>
  #    forward.input.conf: |-
  #      # Takes the messages sent over TCP
  #      <source>
  #        @type forward
  #      </source>
  #    monitoring.conf: |-
  #      # Prometheus Exporter Plugin
  #      # input plugin that exports metrics
  #      <source>
  #        @type prometheus
  #      </source>
  #      <source>
  #        @type monitor_agent
  #      </source>
  #      # input plugin that collects metrics from MonitorAgent
  #      <source>
  #        @type prometheus_monitor
  #        <labels>
  #          host ${hostname}
  #        </labels>
  #      </source>
  #      # input plugin that collects metrics for output plugin
  #      <source>
  #        @type prometheus_output_monitor
  #        <labels>
  #          host ${hostname}
  #        </labels>
  #      </source>
  #      # input plugin that collects metrics for in_tail plugin
  #      <source>
  #        @type prometheus_tail_monitor
  #        <labels>
  #          host ${hostname}
  #        </labels>
  #      </source>
  #    output.conf: |
  #      # Enriches records with Kubernetes metadata
  #      <filter kubernetes.**>
  #        @type kubernetes_metadata
  #      </filter>
  #      <match **>
  #        @id elasticsearch
  #        @type elasticsearch
  #        @log_level info
  #        include_tag_key true
  #        type_name _doc
  #        host "#{ENV['OUTPUT_HOST']}"
  #        port "#{ENV['OUTPUT_PORT']}"
  #        scheme "#{ENV['OUTPUT_SCHEME']}"
  #        ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
  #        logstash_format true
  #        logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
  #        reconnect_on_error true
  #        <buffer>
  #          @type file
  #          path /var/log/fluentd-buffers/kubernetes.system.buffer
  #          flush_mode interval
  #          retry_type exponential_backoff
  #          flush_thread_count 2
  #          flush_interval 5s
  #          retry_forever
  #          retry_max_interval 30
  #          chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
  #          queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
  #          overflow_action block
  #        </buffer>
  #      </match>
  # extraVolumes:
  #   - name: es-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: es-certs
  # extraVolumeMounts:
  #   - name: es-certs
  #     mountPath: /certs
  #     readOnly: true

elasticsearch:
  # Default values for elasticsearch.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  clusterName: "elasticsearch"
  nodeGroup: "master"

  # The service that non master groups will try to connect to when joining the cluster
  # This should be set to clusterName + "-" + nodeGroup for your master group
  masterService: ""

  # Elasticsearch roles that will be applied to this nodeGroup
  # These will be set as environment variables. E.g. node.master=true
  roles:
    master: "true"
    ingest: "true"
    data: "true"

  replicas: 3
  minimumMasterNodes: 2

  esMajorVersion: ""

  # Allows you to add any config files in /usr/share/elasticsearch/config/
  # such as elasticsearch.yml and log4j2.properties
  esConfig: {}
  #  elasticsearch.yml: |
  #    key:
  #      nestedkey: value
  #  log4j2.properties: |
  #    key = value

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/elasticsearch/config/certs

  image: "docker.elastic.co/elasticsearch/elasticsearch"
  imageTag: "7.2.0"
  imagePullPolicy: "IfNotPresent"

  podAnnotations: {}
  # iam.amazonaws.com/role: es-cluster

  esJavaOpts: "-Xmx1g -Xms1g"

  resources:
    requests:
      cpu: "100m"
      memory: "2Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  initResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

  sidecarResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

  networkHost: "0.0.0.0"

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: "awsgp2"
    resources:
      requests:
        storage: 30Gi

  persistence:
    enabled: false
    annotations: {}

  extraVolumes: []
  # - name: extras
  #   emptyDir: {}

  extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

  extraInitContainers: []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # By default this will make sure two pods don't end up on the same node
  # Changing this to a region would allow you to spread pods across regions
  antiAffinityTopologyKey: "kubernetes.io/hostname"

  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "hard"

  # This is the node affinity settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
  nodeAffinity: {}

  # The default is to deploy all pods serially. By setting this to parallel all pods are started at
  # the same time when bootstrapping the cluster
  podManagementPolicy: "Parallel"

  protocol: http
  httpPort: 9200
  transportPort: 9300

  service:
    type: ClusterIP
    nodePort:
    annotations: {}

  updateStrategy: RollingUpdate

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 1

  podSecurityContext:
    fsGroup: 1000

  # The following value is deprecated,
  # please use the above podSecurityContext.fsGroup instead
  fsGroup: ""

  securityContext:
    capabilities:
      drop:
        - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  # How long to wait for elasticsearch to stop gracefully
  terminationGracePeriod: 120

  sysctlVmMaxMapCount: 262144

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  # https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params wait_for_status
  clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

  ## Use an alternate scheduler.
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []

  # Enabling this will publically expose your Elasticsearch instance.
  # Only enable this if you have security enabled on your cluster
  ingress:
    enabled: false
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - elasticsearch.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  nameOverride: ""
  fullnameOverride: ""

  # https://github.com/elastic/helm-charts/issues/63
  masterTerminationFix: false

  lifecycle: {}
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

  sysctlInitContainer:
    enabled: true

kibana:
  elasticsearchURL: "" # "http://elasticsearch-master:9200"
  elasticsearchHosts: "http://elasticsearch-master:9200"

  replicas: 1

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/elasticsearch/config/certs

  image: "docker.elastic.co/kibana/kibana"
  imageTag: "7.2.0"
  imagePullPolicy: "IfNotPresent"

  podAnnotations: {}
  # iam.amazonaws.com/role: es-cluster

  resources:
    requests:
      cpu: "100m"
      memory: "500m"
    limits:
      cpu: "1000m"
      memory: "1Gi"

  protocol: http

  serverHost: "0.0.0.0"

  healthCheckPath: "/app/kibana"

  # Allows you to add any config files in /usr/share/kibana/config/
  # such as kibana.yml
  kibanaConfig: {}
  #   kibana.yml: |
  #     key:
  #       nestedkey: value

  # If Pod Security Policy in use it may be required to specify security context as well as service account

  podSecurityContext:
    fsGroup: 1000

  securityContext:
    capabilities:
      drop:
        - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  serviceAccount: ""

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # By default this will make sure two pods don't end up on the same node
  # Changing this to a region would allow you to spread pods across regions
  antiAffinityTopologyKey: "kubernetes.io/hostname"

  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "hard"

  httpPort: 5601

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 1

  updateStrategy:
    type: "Recreate"

  service:
    type: ClusterIP
    port: 5601
    nodePort:
    annotations: {}
      # cloud.google.com/load-balancer-type: "Internal"
      # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
    # service.beta.kubernetes.io/azure-load-balancer-internal: "true"
    # service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
    # service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"

  ingress:
    enabled: true
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - kibana.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []
  affinity: {}

  nameOverride: ""
  fullnameOverride: ""

apm-server:
  image:
    repository: docker.elastic.co/apm/apm-server
    tag: 7.0.0
    pullPolicy: IfNotPresent

  # DaemonSet or Deployment
  kind: DaemonSet

  # Number of replicas when kind is Deployment
  replicaCount: 1

  # Create HorizontalPodAutoscaler object when kind is Deployment
  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          targetAverageUtilization: 60
      - type: Resource
        resource:
          name: memory
          targetAverageUtilization: 60

  # The update strategy to apply to the Deployment or DaemonSet
  updateStrategy: {}
  # rollingUpdate:
  #   maxUnavailable: 1
  # type: RollingUpdate

  service:
    enabled: true
    type: ClusterIP
    port: 8200
    # portName: apm-server-svc
    # clusterIP: None
    ## External IP addresses of service
    ## Default: nil
    # externalIPs:
    # - 192.168.0.1
    #
    ## LoadBalancer IP if service.type is LoadBalancer
    ## Default: nil
    # loadBalancerIP: 10.2.2.2
    ## Limit load balancer source ips to list of CIDRs (where available)
    # loadBalancerSourceRanges: []

    annotations: {}
    # Annotation example: setup ssl with aws cert when service.type is LoadBalancer
    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:EXAMPLE_CERT
    labels: {}
    ## Label example: show service URL in `kubectl cluster-info`
    # kubernetes.io/cluster-service: "true"

  ingress:
    enabled: false
    annotations: {}
    # Annotation example: set nginx ingress type
    # kubernetes.io/ingress.class: nginx-public
    labels: {}
    hosts:
      - apm-server.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  config:
    apm-server:
      ### Defines the host and port the server is listening on
      host: "0.0.0.0:8200"

      ## Maximum permitted size in bytes of an unzipped request accepted by the server to be processed.
      # max_unzipped_size: 52428800
      ## Maximum permitted size in bytes of a request's header accepted by the server to be processed.
      # max_header_size: 1048576

      ## Maximum permitted duration in seconds for reading an entire request.
      # read_timeout: 2s
      ## Maximum permitted duration in seconds for writing a response.
      # write_timeout: 2s

      ## Maximum duration in seconds before releasing resources when shutting down the server.
      # shutdown_timeout: 5s

      ## Maximum number of requests permitted to be sent to the server concurrently.
      # concurrent_requests: 40

      ## Authorization token to be checked. If a token is set here the agents must
      ## send their token in the following format: Authorization: Bearer <secret-token>.
      ## It is recommended to use an authorization token in combination with SSL enabled.
      # secret_token:
      # ssl.enabled: false
      # ssl.certificate : "path/to/cert"
      # ssl.key : "path/to/private_key"

    queue: {}
      ## Queue type by name (default 'mem')
      ## The memory queue will present all available events (up to the outputs
      ## bulk_max_size) to the output, the moment the output is ready to server
      ## another batch of events.
      # mem:
      ## Max number of events the queue can buffer.
      # events: 4096

      ## Hints the minimum number of events stored in the queue,
      ## before providing a batch of events to the outputs.
      ## A value of 0 (the default) ensures events are immediately available
    ## to be sent to the outputs.
    # flush.min_events: 2048

    ## Maximum duration after which events are available to the outputs,
    ## if the number of events stored in the queue is < min_flush_events.
    # flush.timeout: 1s

    # When a key contains a period, use this format for setting values on the command line:
    # --set config."output\.file".enabled=false
    output.file:
      enabled: false
    #      path: "/usr/share/apm-server/data"
    #      filename: apm-server
    #      rotate_every_kb: 10000
    #      number_of_files: 5

    ## Set output.file.enabled to false to enable elasticsearch
    output.elasticsearch:
      hosts: ["elasticsearch-master:9200"]
      protocol: "http"
  #      protocol: "https"
  #      username: "elastic"
  #      password: "changeme"

  # List of beat plugins
  plugins: []
  # - kinesis.so

  # Additional container arguments
  extraArgs: []
  # - -d
  # - *

  # A map of additional environment variables
  extraVars: {}
  # test1: "test2"

  # Add additional volumes and mounts, for example to read other log files on the host
  extraVolumes: []
  # - hostPath:
  #     path: /var/log
  #   name: varlog
  extraVolumeMounts: []
  # - name: varlog
  #   mountPath: /host/var/log
  #   readOnly: true

  ## Labels to be added to pods
  podLabels: {}

  ## Annotations to be added to pods
  podAnnotations: {}

  resources: {}
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
  #  cpu: 100m
  #  memory: 200Mi
  # requests:
  #  cpu: 100m
  #  memory: 100Mi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  # Tolerations for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Affinity configuration for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  affinity: {}

  rbac:
    # Specifies whether RBAC resources should be created
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:
